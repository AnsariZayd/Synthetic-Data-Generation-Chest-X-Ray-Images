{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":1317101,"sourceType":"datasetVersion","datasetId":642388},{"sourceId":9959397,"sourceType":"datasetVersion","datasetId":6125691}],"dockerImageVersionId":30787,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install pyngrok diffusers peft\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-25T06:11:08.985606Z","iopub.execute_input":"2024-11-25T06:11:08.985919Z","iopub.status.idle":"2024-11-25T06:11:20.548103Z","shell.execute_reply.started":"2024-11-25T06:11:08.985890Z","shell.execute_reply":"2024-11-25T06:11:20.547271Z"}},"outputs":[{"name":"stdout","text":"Collecting pyngrok\n  Downloading pyngrok-7.2.1-py3-none-any.whl.metadata (8.3 kB)\nCollecting diffusers\n  Downloading diffusers-0.31.0-py3-none-any.whl.metadata (18 kB)\nCollecting peft\n  Downloading peft-0.13.2-py3-none-any.whl.metadata (13 kB)\nRequirement already satisfied: PyYAML>=5.1 in /opt/conda/lib/python3.10/site-packages (from pyngrok) (6.0.2)\nRequirement already satisfied: importlib-metadata in /opt/conda/lib/python3.10/site-packages (from diffusers) (7.0.0)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from diffusers) (3.15.1)\nRequirement already satisfied: huggingface-hub>=0.23.2 in /opt/conda/lib/python3.10/site-packages (from diffusers) (0.25.1)\nRequirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (from diffusers) (1.26.4)\nRequirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from diffusers) (2024.5.15)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from diffusers) (2.32.3)\nRequirement already satisfied: safetensors>=0.3.1 in /opt/conda/lib/python3.10/site-packages (from diffusers) (0.4.5)\nRequirement already satisfied: Pillow in /opt/conda/lib/python3.10/site-packages (from diffusers) (10.3.0)\nRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from peft) (21.3)\nRequirement already satisfied: psutil in /opt/conda/lib/python3.10/site-packages (from peft) (5.9.3)\nRequirement already satisfied: torch>=1.13.0 in /opt/conda/lib/python3.10/site-packages (from peft) (2.4.0)\nRequirement already satisfied: transformers in /opt/conda/lib/python3.10/site-packages (from peft) (4.45.1)\nRequirement already satisfied: tqdm in /opt/conda/lib/python3.10/site-packages (from peft) (4.66.4)\nRequirement already satisfied: accelerate>=0.21.0 in /opt/conda/lib/python3.10/site-packages (from peft) (0.34.2)\nRequirement already satisfied: fsspec>=2023.5.0 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.23.2->diffusers) (2024.6.1)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.23.2->diffusers) (4.12.2)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.0->peft) (3.1.2)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch>=1.13.0->peft) (1.13.3)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch>=1.13.0->peft) (3.3)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch>=1.13.0->peft) (3.1.4)\nRequirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.10/site-packages (from importlib-metadata->diffusers) (3.19.2)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->diffusers) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->diffusers) (3.7)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->diffusers) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->diffusers) (2024.8.30)\nRequirement already satisfied: tokenizers<0.21,>=0.20 in /opt/conda/lib/python3.10/site-packages (from transformers->peft) (0.20.0)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch>=1.13.0->peft) (2.1.5)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/conda/lib/python3.10/site-packages (from sympy->torch>=1.13.0->peft) (1.3.0)\nDownloading pyngrok-7.2.1-py3-none-any.whl (22 kB)\nDownloading diffusers-0.31.0-py3-none-any.whl (2.9 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.9/2.9 MB\u001b[0m \u001b[31m35.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading peft-0.13.2-py3-none-any.whl (320 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m320.7/320.7 kB\u001b[0m \u001b[31m21.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hInstalling collected packages: pyngrok, diffusers, peft\nSuccessfully installed diffusers-0.31.0 peft-0.13.2 pyngrok-7.2.1\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"from pyngrok import ngrok\n\n\nngrok.set_auth_token(\"2p6aR2UdbsZyxb8oknlZ6RFZljv_4Kkbd8Yr83gZB5N3YdVjF\")\n","metadata":{"execution":{"iopub.status.busy":"2024-11-25T06:11:20.704928Z","iopub.execute_input":"2024-11-25T06:11:20.705237Z","iopub.status.idle":"2024-11-25T06:11:21.612610Z","shell.execute_reply.started":"2024-11-25T06:11:20.705194Z","shell.execute_reply":"2024-11-25T06:11:21.611745Z"},"trusted":true},"outputs":[{"name":"stdout","text":"                                                                                                    \r","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"from flask import Flask, request, jsonify\nfrom io import BytesIO\nimport base64\nfrom PIL import Image\nimport torch\nfrom diffusers import StableDiffusionPipeline\nfrom pyngrok import ngrok\n\napp = Flask(__name__)\n\n\ndef load_model_with_lora_weights(lora_weights_path, model_name=\"runwayml/stable-diffusion-v1-5\"):\n    try:\n        \n        pipe = StableDiffusionPipeline.from_pretrained(\n            model_name,\n            torch_dtype=torch.float16\n        ).to(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n        print(\"Base model loaded successfully.\")\n\n        \n        print(\"Loading LoRA weights...\")\n        pipe.load_lora_weights(lora_weights_path)\n        print(\"LoRA weights applied successfully.\")\n\n        return pipe\n    except Exception as e:\n        print(f\"Error loading model or LoRA weights: {e}\")\n        raise\n\n\nlora_weights_path = \"/kaggle/input/lora-weights/pytorch_lora_weights (1000).safetensors\"\n\n\npipe = load_model_with_lora_weights(lora_weights_path)\n\n\ndef generate(pipeline, prompt, seed, num_inference_steps=35, num_images=1):\n    \"\"\"\n    Generate one or more unique images using a Stable Diffusion pipeline.\n\n    Args:\n        pipeline: The DiffusionPipeline object.\n        prompt (str): The text prompt to generate images.\n        seed (int): The initial seed for reproducibility.\n        num_inference_steps (int): The number of inference steps. Default is 35.\n        num_images (int): The number of unique images to generate. Default is 1.\n\n    Returns:\n        List of generated images.\n    \"\"\"\n    \n    if isinstance(prompt, str):\n        prompt = [prompt] * num_images  # Duplicate the prompt for batch generation\n\n    images = []\n    for i in range(num_images):\n        \n        generator = torch.Generator(device=\"cuda\").manual_seed(seed + i)\n\n        \n        result = pipeline(prompt[i % len(prompt)], num_inference_steps=num_inference_steps, generator=generator)\n        images.append(result.images[0])\n\n    return images\n\n\n@app.route(\"/generate\", methods=[\"POST\"])\ndef generate_image():\n    try:\n        \n        data = request.json\n        prompt = data.get(\"prompt\")\n        seed = data.get(\"seed\", 42)\n        num_inference_steps = data.get(\"num_inference_steps\", 35)\n        num_images = data.get(\"num_images\", 1)\n\n        if not prompt:\n            return jsonify({\"error\": \"Prompt is required\"}), 400\n\n        \n        images = generate(pipe, prompt, seed, num_inference_steps, num_images)\n\n       \n        encoded_images = []\n        for image in images:\n            buffered = BytesIO()\n            image.save(buffered, format=\"PNG\")\n            img_str = base64.b64encode(buffered.getvalue()).decode(\"utf-8\")\n            encoded_images.append(img_str)\n\n        \n        return jsonify({\"images\": encoded_images})\n    except Exception as e:\n        return jsonify({\"error\": str(e)}), 500\n\n\nif __name__ == \"__main__\":\n    try:\n        \n        public_url = ngrok.connect(5000)\n        print(f\"Public URL: {public_url}\")\n\n        \n        app.run(port=5000)\n\n    except Exception as e:\n        print(f\"Error starting Flask app: {e}\")\n    finally:\n        \n        ngrok.disconnect(public_url)\n        print(\"ngrok tunnel closed.\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-25T06:11:24.991308Z","iopub.execute_input":"2024-11-25T06:11:24.992070Z"}},"outputs":[{"name":"stderr","text":"The cache for model files in Transformers v4.22.0 has been updated. Migrating your old cache. This is a one-time only operation. You can interrupt this and resume the migration later on by calling `transformers.utils.move_cache()`.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"0it [00:00, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e42cf46d0000445aa5bfb67127669eb3"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model_index.json:   0%|          | 0.00/541 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6b5253aefbad4ccb8f66d84adb06704b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Fetching 15 files:   0%|          | 0/15 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8021c954769c4a45813c06a8f1e06d8b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"text_encoder/config.json:   0%|          | 0.00/617 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b32e6381ef0d44cc8679f74084271a74"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer/merges.txt:   0%|          | 0.00/525k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9043928b97fe4366af0869b875833a2f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer/special_tokens_map.json:   0%|          | 0.00/472 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"648c73981394479d8754501f92dcd4d9"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"(…)ature_extractor/preprocessor_config.json:   0%|          | 0.00/342 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9a45ca96d8704b3c8ea153d10318e784"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"scheduler/scheduler_config.json:   0%|          | 0.00/308 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2d767a524ce3434da7bf302047696f1f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/492M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3dc18ce7a78e4e4598aef50c8d05d284"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/1.22G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c67c4c917c7a402f8b0e16e07e9efeb7"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"safety_checker/config.json:   0%|          | 0.00/4.72k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0bd2d96423924af3b01ed9db7d39693c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer/tokenizer_config.json:   0%|          | 0.00/806 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"82a41423ebbb4c31bec9686f3010842b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"diffusion_pytorch_model.safetensors:   0%|          | 0.00/3.44G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8752429fafcb46c79b1b2be0b982baf7"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"unet/config.json:   0%|          | 0.00/743 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"45cca96f6501415fbb57f62c89bb2cf4"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer/vocab.json:   0%|          | 0.00/1.06M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"610290a2247c4839953faf120177f62f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vae/config.json:   0%|          | 0.00/547 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"11556906d65d4867962fbd65f54c67f2"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"diffusion_pytorch_model.safetensors:   0%|          | 0.00/335M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8f6fe4b1da444d8f9c54cb60b5f63e63"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Loading pipeline components...:   0%|          | 0/7 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4052ba9869c04f3a98eb028caff4d116"}},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1617: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be deprecated in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"Base model loaded successfully.\nLoading LoRA weights...\nLoRA weights applied successfully.\nPublic URL: NgrokTunnel: \"https://163e-34-31-102-43.ngrok-free.app\" -> \"http://localhost:5000\"\n * Serving Flask app '__main__'\n * Debug mode: off\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/35 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d46246e43ef94935a572ec3656659b21"}},"metadata":{}}],"execution_count":null}]}